{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from elembeddings.elembedding import (\n",
    "    ELModel, load_data, load_valid_data, Generator, MyModelCheckpoint)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "embedding_size = 50\n",
    "margin = -0.01\n",
    "reg_norm = 1\n",
    "learning_rate = 3e-4\n",
    "epochs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51671 10\n"
     ]
    }
   ],
   "source": [
    "train_data, classes, relations = load_data('data/data-train/yeast-classes-normalized.owl')\n",
    "valid_data = load_valid_data('data/data-valid/4932.protein.links.v10.5.txt', classes, relations)\n",
    "    \n",
    "proteins = {}\n",
    "for k, v in classes.items():\n",
    "    if not k.startswith('<http://purl.obolibrary.org/obo/GO_'):\n",
    "        proteins[k] = v\n",
    "\n",
    "nb_classes = len(classes)\n",
    "nb_relations = len(relations)\n",
    "nb_train_data = 0\n",
    "for key, val in train_data.items():\n",
    "    nb_train_data = max(len(val), nb_train_data)\n",
    "train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
    "train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
    "\n",
    "cls_dict = {v: k for k, v in classes.items()}\n",
    "rel_dict = {v: k for k, v in relations.items()}\n",
    "\n",
    "cls_list = []\n",
    "rel_list = []\n",
    "for i in range(nb_classes):\n",
    "    cls_list.append(cls_dict[i])\n",
    "for i in range(nb_relations):\n",
    "    rel_list.append(rel_dict[i])\n",
    "\n",
    "        \n",
    "print(nb_classes, nb_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kulmanm/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "10733/10734 [============================>.] - ETA: 0s - loss: 45.7969\n",
      " Saving embeddings 1 1772.2059329120982\n",
      "\n",
      "10734/10734 [==============================] - 144s 13ms/step - loss: 45.7939\n",
      "Epoch 2/128\n",
      "10733/10734 [============================>.] - ETA: 0s - loss: 4.8623\n",
      " Saving embeddings 2 1768.0118937000557\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 4.8619\n",
      "Epoch 3/128\n",
      "10732/10734 [============================>.] - ETA: 0s - loss: 0.8944\n",
      " Saving embeddings 3 1729.2357600817693\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.8944\n",
      "Epoch 4/128\n",
      "10731/10734 [============================>.] - ETA: 0s - loss: 0.5113\n",
      " Saving embeddings 4 1604.8418974168371\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.5112\n",
      "Epoch 5/128\n",
      "10732/10734 [============================>.] - ETA: 0s - loss: 0.3158\n",
      " Saving embeddings 5 1201.4964922876788\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.3158\n",
      "Epoch 6/128\n",
      "10731/10734 [============================>.] - ETA: 0s - loss: 0.2151\n",
      " Saving embeddings 6 770.5193737223565\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.2150\n",
      "Epoch 7/128\n",
      "10731/10734 [============================>.] - ETA: 0s - loss: 0.1570\n",
      " Saving embeddings 7 592.4931471845382\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.1570\n",
      "Epoch 8/128\n",
      "10732/10734 [============================>.] - ETA: 0s - loss: 0.1233\n",
      " Saving embeddings 8 516.6255807470731\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.1233\n",
      "Epoch 9/128\n",
      "10732/10734 [============================>.] - ETA: 0s - loss: 0.1016\n",
      " Saving embeddings 9 440.44041535030664\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.1016\n",
      "Epoch 10/128\n",
      "10733/10734 [============================>.] - ETA: 0s - loss: 0.0868\n",
      " Saving embeddings 10 400.3001765471102\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0868\n",
      "Epoch 11/128\n",
      "10730/10734 [============================>.] - ETA: 0s - loss: 0.0759\n",
      " Saving embeddings 11 377.556448615499\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0759\n",
      "Epoch 12/128\n",
      "10729/10734 [============================>.] - ETA: 0s - loss: 0.0691\n",
      " Saving embeddings 12 346.17055380040887\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0691\n",
      "Epoch 13/128\n",
      "10732/10734 [============================>.] - ETA: 0s - loss: 0.0635\n",
      " Saving embeddings 13 335.59726351979185\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0635\n",
      "Epoch 14/128\n",
      "10730/10734 [============================>.] - ETA: 0s - loss: 0.0579\n",
      " Saving embeddings 14 321.42296970823264\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0579\n",
      "Epoch 15/128\n",
      "10729/10734 [============================>.] - ETA: 0s - loss: 0.0540\n",
      " Saving embeddings 15 319.568272625906\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0540\n",
      "Epoch 16/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0510\n",
      "Epoch 17/128\n",
      "10730/10734 [============================>.] - ETA: 0s - loss: 0.0480\n",
      " Saving embeddings 17 296.80201170786097\n",
      "\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0480\n",
      "Epoch 18/128\n",
      "10734/10734 [==============================] - 140s 13ms/step - loss: 0.0457\n",
      "Epoch 19/128\n",
      "10734/10734 [==============================] - 140s 13ms/step - loss: 0.0440\n",
      "Epoch 20/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0426\n",
      "Epoch 21/128\n",
      "10734/10734 [==============================] - 139s 13ms/step - loss: 0.0413\n",
      "Epoch 22/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0403\n",
      "Epoch 23/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0392\n",
      "Epoch 24/128\n",
      "10731/10734 [============================>.] - ETA: 0s - loss: 0.0382\n",
      " Saving embeddings 24 292.96443504924736\n",
      "\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0382\n",
      "Epoch 25/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0365\n",
      "Epoch 26/128\n",
      "10732/10734 [============================>.] - ETA: 0s - loss: 0.0365\n",
      " Saving embeddings 26 289.08938858948153\n",
      "\n",
      "10734/10734 [==============================] - 143s 13ms/step - loss: 0.0365\n",
      "Epoch 27/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0355\n",
      "Epoch 28/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0354\n",
      "Epoch 29/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0339\n",
      "Epoch 30/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0333\n",
      "Epoch 31/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0318\n",
      "Epoch 32/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0326\n",
      "Epoch 33/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0323\n",
      "Epoch 34/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0313\n",
      "Epoch 35/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0306\n",
      "Epoch 36/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0304\n",
      "Epoch 37/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0308\n",
      "Epoch 38/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0302\n",
      "Epoch 39/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0288\n",
      "Epoch 40/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0296\n",
      "Epoch 41/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0293\n",
      "Epoch 42/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0286\n",
      "Epoch 43/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0280\n",
      "Epoch 44/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0282\n",
      "Epoch 45/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0271\n",
      "Epoch 46/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0281\n",
      "Epoch 47/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0269\n",
      "Epoch 48/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0279\n",
      "Epoch 49/128\n",
      "10734/10734 [==============================] - 143s 13ms/step - loss: 0.0266\n",
      "Epoch 50/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0264\n",
      "Epoch 51/128\n",
      "10734/10734 [==============================] - 143s 13ms/step - loss: 0.0254\n",
      "Epoch 52/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0259\n",
      "Epoch 53/128\n",
      "10734/10734 [==============================] - 141s 13ms/step - loss: 0.0264\n",
      "Epoch 54/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0255\n",
      "Epoch 55/128\n",
      "10734/10734 [==============================] - 143s 13ms/step - loss: 0.0252\n",
      "Epoch 56/128\n",
      "10734/10734 [==============================] - 143s 13ms/step - loss: 0.0239\n",
      "Epoch 57/128\n",
      "10734/10734 [==============================] - 143s 13ms/step - loss: 0.0239\n",
      "Epoch 58/128\n",
      "10734/10734 [==============================] - 144s 13ms/step - loss: 0.0229\n",
      "Epoch 59/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0229\n",
      "Epoch 60/128\n",
      "10734/10734 [==============================] - 142s 13ms/step - loss: 0.0224\n",
      "Epoch 61/128\n",
      " 1812/10734 [====>.........................] - ETA: 1:37 - loss: 0.0239"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f5b1a3cbe18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     callbacks=[checkpointer,])\n\u001b[0m",
      "\u001b[0;32m~/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nf1 = Input(shape=(2,), dtype=np.int32)\n",
    "nf2 = Input(shape=(3,), dtype=np.int32)\n",
    "nf3 = Input(shape=(3,), dtype=np.int32)\n",
    "nf4 = Input(shape=(3,), dtype=np.int32)\n",
    "dis = Input(shape=(3,), dtype=np.int32)\n",
    "top = Input(shape=(1,), dtype=np.int32)\n",
    "nf3_neg = Input(shape=(3,), dtype=np.int32)\n",
    "el_model = ELModel(nb_classes, nb_relations, embedding_size, batch_size, margin, reg_norm)\n",
    "out = el_model([nf1, nf2, nf3, nf4, dis, top, nf3_neg])\n",
    "model = tf.keras.Model(inputs=[nf1, nf2, nf3, nf4, dis, top, nf3_neg], outputs=out)\n",
    "optimizer = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "out_classes_file = 'data/cls_embeddings.pkl'\n",
    "out_relations_file = 'data/rel_embeddings.pkl'\n",
    "\n",
    "checkpointer = MyModelCheckpoint(\n",
    "    out_classes_file=out_classes_file,\n",
    "    out_relations_file=out_relations_file,\n",
    "    cls_list=cls_list,\n",
    "    rel_list=rel_list,\n",
    "    valid_data=valid_data,\n",
    "    proteins=proteins,\n",
    "    monitor='loss')\n",
    "        \n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    workers=12,\n",
    "    callbacks=[checkpointer,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 5504 samples in 0.009s...\n",
      "[t-SNE] Computed neighbors for 5504 samples in 2.937s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 5504\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 5504\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 5504\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 5504\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 5504\n",
      "[t-SNE] Computed conditional probabilities for sample 5504 / 5504\n",
      "[t-SNE] Mean sigma: 0.304443\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 89.340729\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "cls_df = pd.read_pickle('data/cls_embeddings.pkl')\n",
    "rel_df = pd.read_pickle('data/rel_embeddings.pkl')\n",
    "\n",
    "cls_embeddings = cls_df['embeddings'].values\n",
    "rel_embeddings = rel_df['embeddings'].values\n",
    "\n",
    "n = len(proteins)\n",
    "embedding_size = cls_embeddings[0].shape[0] - 1\n",
    "embeds = np.zeros((n, embedding_size))\n",
    "for i, item in enumerate(proteins.items()):\n",
    "    k, v = item\n",
    "    embeds[i, :] = cls_embeddings[v][:-1]\n",
    "\n",
    "X = TSNE(n_components=2, verbose=1).fit_transform(embeds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "\n",
    "# Load EC numbers\n",
    "ec_numbers = {}\n",
    "with open('data/yeast_ec.tab') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        it = line.strip().split('\\t', -1)\n",
    "        if len(it) < 5:\n",
    "            continue\n",
    "        if it[3]:\n",
    "            prot_id = it[3].split(';')[0]\n",
    "            prot_id = '<http://{0}>'.format(prot_id)    \n",
    "            ec_numbers[prot_id] = it[4]\n",
    "\n",
    "classes = {'0': [[], []]}\n",
    "for i, item in enumerate(proteins.items()):\n",
    "    k, v = item\n",
    "    if k in ec_numbers:\n",
    "        ec = ec_numbers[k].split('.')[0]\n",
    "        if ec not in classes:\n",
    "            classes[ec] = [[], []]\n",
    "        classes[ec][0].append(X[i, 0])\n",
    "        classes[ec][1].append(X[i, 1])\n",
    "    else:\n",
    "        classes['0'][0].append(X[i, 0])\n",
    "        classes['0'][1].append(X[i, 1])\n",
    "    \n",
    "colors = iter(cm.rainbow(np.linspace(0, 1, len(classes))))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for ec, items in classes.items():\n",
    "    if ec == '0':\n",
    "        continue\n",
    "    color = next(colors)\n",
    "    ax.scatter(items[0], items[1], color=color, label=ec)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
